{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1vMC-2_cLMpllgN24HP55nmHlH_SReiFF",
      "authorship_tag": "ABX9TyN5YNYghXwIgNqZIyt2vvpD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/envomp/predicate_logic_training/blob/main/predicate_logic_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einx dataclasses_json llama_models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_GYWJOyqu57",
        "outputId": "582c1fa8-00b6-4bf8-938c-b162722df149"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einx in /usr/local/lib/python3.10/dist-packages (0.3.0)\n",
            "Requirement already satisfied: dataclasses_json in /usr/local/lib/python3.10/dist-packages (0.6.7)\n",
            "Collecting llama_models\n",
            "  Downloading llama_models-0.0.55-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from einx) (1.26.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from einx) (1.13.1)\n",
            "Requirement already satisfied: frozendict in /usr/local/lib/python3.10/dist-packages (from einx) (2.4.6)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses_json) (3.23.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses_json) (0.9.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from llama_models) (6.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from llama_models) (3.1.4)\n",
            "Collecting tiktoken (from llama_models)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: pydantic>=2 in /usr/local/lib/python3.10/dist-packages (from llama_models) (2.9.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from llama_models) (11.0.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses_json) (24.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2->llama_models) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2->llama_models) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2->llama_models) (4.12.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses_json) (1.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->llama_models) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->einx) (1.3.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->llama_models) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->llama_models) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->llama_models) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->llama_models) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->llama_models) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->llama_models) (2024.8.30)\n",
            "Downloading llama_models-0.0.55-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, llama_models\n",
            "Successfully installed llama_models-0.0.55 tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "AZecudgyqAJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hierarchical_routing import *\n",
        "import processor\n",
        "from eval import eval_model, visualize_routes\n",
        "from data_preprocessing import pad_collate"
      ],
      "metadata": {
        "id": "VaqEiio5qTQv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for_classification = False\n",
        "train_ds = processor.load(\"../dataset/predicate_logic/train/70k/prop_examples_lp.txt\", for_classification=for_classification)\n",
        "validation_ds = processor.load(\"../dataset/predicate_logic/validation/prop_examples_lp.txt\", for_classification=for_classification)\n",
        "inference_ids = processor.load(\"../dataset/predicate_logic/validation/prop_examples_lp.txt\", for_classification=for_classification)\n",
        "\n",
        "def ds_loader(ds, ds_length, epoch):\n",
        "    # curriculum = processor.train_curriculum(ds, epoch, select_layer_items=ds_length // 7, non_select_layer_items=ds_length // 140)\n",
        "    return DataLoader(ds, shuffle=True, batch_size=12, collate_fn=lambda x: pad_collate(x, padding=processor.pad))\n",
        "\n",
        "def loss_fn(llm_output, depth):\n",
        "    logits, labels, gating, history = llm_output\n",
        "\n",
        "    if for_classification:\n",
        "        logit = logits[:, -1]\n",
        "        total_loss = F.binary_cross_entropy_with_logits(logit, labels)\n",
        "    else:\n",
        "        total_loss = F.cross_entropy(logits.transpose(1, 2), labels, label_smoothing=0.)\n",
        "\n",
        "    # layers_traversed_loss = 0.0\n",
        "    # for next_layer_prob in gating:\n",
        "    #     layers_traversed_loss += F.binary_cross_entropy(next_layer_prob, torch.ones_like(next_layer_prob))\n",
        "\n",
        "    # # whatever you think, be confident in it\n",
        "    # confidence_loss = 0.0\n",
        "    # for next_layer_prob in gating:\n",
        "    #     confidence = 0.5 - torch.abs(next_layer_prob - 0.5)\n",
        "    #     confidence_loss += confidence.mean()\n",
        "    # confidence_loss *= 0.01\n",
        "    # confidence_loss /= len(gating)\n",
        "    # total_loss += confidence_loss\n",
        "\n",
        "    # fixed routing\n",
        "    total_routing_loss = 0\n",
        "    for pos, gates in enumerate(gating):\n",
        "        routing_loss = 0\n",
        "        for i in range(len(gates)):\n",
        "            next_layer_prob = gates[i]\n",
        "            is_forward = is_forward_at_position_for_depth(i, depth[pos])\n",
        "            target_prob = torch.ones_like(next_layer_prob) if is_forward > 0 else torch.zeros_like(next_layer_prob)\n",
        "            routing_loss += F.binary_cross_entropy(next_layer_prob, target_prob)\n",
        "        total_routing_loss += routing_loss / len(gates)\n",
        "    total_loss += total_routing_loss / len(gating)\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "def inference_model(llm_model, epoch=0):\n",
        "    if for_classification:\n",
        "        vocabulary, answer_position = {1: 1, 0: 0}, 1\n",
        "    else:\n",
        "        vocabulary, answer_position = processor.special_tokens, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        routes = []\n",
        "        def model_invocation(xs, labels, depth, global_attention_mask, **kwargs):\n",
        "            labels = torch.tensor([labels]).to(\"cuda\")\n",
        "            depths = torch.tensor([depth]).to(\"cuda\")\n",
        "            global_attention_mask = torch.tensor([global_attention_mask]).to(\"cuda\")\n",
        "            logits, labels, gates, history = llm_model(xs, labels=labels, depths=depths, global_attention_mask=global_attention_mask)\n",
        "            routes.append([x.item() for x in history])\n",
        "            return logits\n",
        "        # eval_model(model_invocation, [x for x in inference_ids if x[\"depth\"] <= epoch], vocabulary=vocabulary, answer_position=answer_position)\n",
        "        eval_model(model_invocation, inference_ids, vocabulary=vocabulary, answer_position=answer_position)\n",
        "        visualize_routes(routes)\n",
        "\n",
        "def call_model(llm, batch, loss_fn):\n",
        "    input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
        "    labels = batch[\"labels\"].to(\"cuda\")\n",
        "    depths = batch[\"depth\"].to(\"cuda\")\n",
        "    global_attention_mask = batch[\"global_attention_mask\"].to(\"cuda\")\n",
        "\n",
        "    llm_output = llm.forward_train(input_ids, None if for_classification else labels, depths=depths, global_attention_mask=global_attention_mask)\n",
        "    return loss_fn(llm_output, depths)\n",
        "\n",
        "llm_model = create_model(root_path, ModelArgs, Transformer)\n",
        "# optimizer = AdamWScheduleFree(llm_model.parameters(), weight_decay=0.05, betas=(0.9, 0.98), lr=1e-4, warmup_steps=2000)\n",
        "optimizer = torch.optim.Adam(llm_model.parameters(), weight_decay=0.1, betas=(0.9, 0.98), lr=1e-5)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
        "train_conf = TrainConf(epochs=10, optimizer=optimizer, scheduler=scheduler, loss_fn=loss_fn, ds_loader=ds_loader, eval_model=inference_model)\n",
        "\n",
        "visualize(train(train_conf, llm_model, train_ds, validation_ds, model_call=call_model))"
      ],
      "metadata": {
        "id": "AS1WZ0xUsJgp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}