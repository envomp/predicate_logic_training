{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1vMC-2_cLMpllgN24HP55nmHlH_SReiFF",
      "authorship_tag": "ABX9TyNjC4o0OXCfYUZcwT79oj5G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/envomp/predicate_logic_training/blob/main/predicate_logic_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einx dataclasses_json llama_models blobfile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_GYWJOyqu57",
        "outputId": "9b078ec3-7b19-4126-d8cc-4e4a6f165731"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einx\n",
            "  Downloading einx-0.3.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting dataclasses_json\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting llama_models\n",
            "  Downloading llama_models-0.0.55-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting blobfile\n",
            "  Downloading blobfile-3.0.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from einx) (1.26.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from einx) (1.13.1)\n",
            "Requirement already satisfied: frozendict in /usr/local/lib/python3.10/dist-packages (from einx) (2.4.6)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses_json)\n",
            "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses_json)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from llama_models) (6.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from llama_models) (3.1.4)\n",
            "Collecting tiktoken (from llama_models)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: pydantic>=2 in /usr/local/lib/python3.10/dist-packages (from llama_models) (2.9.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from llama_models) (11.0.0)\n",
            "Collecting pycryptodomex>=3.8 (from blobfile)\n",
            "  Downloading pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.25.3 in /usr/local/lib/python3.10/dist-packages (from blobfile) (2.2.3)\n",
            "Requirement already satisfied: lxml>=4.9 in /usr/local/lib/python3.10/dist-packages (from blobfile) (5.3.0)\n",
            "Requirement already satisfied: filelock>=3.0 in /usr/local/lib/python3.10/dist-packages (from blobfile) (3.16.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses_json) (24.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2->llama_models) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2->llama_models) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2->llama_models) (4.12.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses_json)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->llama_models) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->einx) (1.3.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->llama_models) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->llama_models) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->llama_models) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->llama_models) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->llama_models) (2024.8.30)\n",
            "Downloading einx-0.3.0-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.0/103.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading llama_models-0.0.55-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blobfile-3.0.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: pycryptodomex, mypy-extensions, marshmallow, typing-inspect, tiktoken, einx, blobfile, llama_models, dataclasses_json\n",
            "Successfully installed blobfile-3.0.0 dataclasses_json-0.6.7 einx-0.3.0 llama_models-0.0.55 marshmallow-3.23.1 mypy-extensions-1.0.0 pycryptodomex-3.21.0 tiktoken-0.8.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "AZecudgyqAJT",
        "outputId": "73bb3959-04a5-4071-fba3-23a0087e681a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from dataclasses_json import dataclass_json\n",
        "from typing import Optional\n",
        "\n",
        "\n",
        "@dataclass_json\n",
        "@dataclass\n",
        "class ModelArgs:\n",
        "    dim: int = 4096\n",
        "    n_layers: int = 32\n",
        "    n_heads: int = 32\n",
        "    n_kv_heads: Optional[int] = None\n",
        "    vocab_size: int = -1\n",
        "    output_size: Optional[int] = None\n",
        "    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n",
        "    ffn_dim_multiplier: Optional[float] = None\n",
        "    meta_embeddings: int = 0\n",
        "\n",
        "    max_batch_size: int = 32\n",
        "    max_seq_len: int = 2048\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        for k, v in kwargs.items():\n",
        "            if hasattr(self, k):\n",
        "                setattr(self, k, v)\n",
        "\n",
        "        if self.n_kv_heads is None:\n",
        "            self.n_kv_heads = self.n_heads\n",
        "        assert self.n_kv_heads <= self.n_heads\n",
        "        assert self.n_heads % self.n_kv_heads == 0\n",
        "        assert self.dim % self.n_heads == 0"
      ],
      "metadata": {
        "id": "S2lJixZfvv0Q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import default_collate\n",
        "import torch\n",
        "\n",
        "\n",
        "def next_word_prediction_labels(input_ids, answer_ids):\n",
        "    labels = [-100 for _ in input_ids] + [x for x in answer_ids]\n",
        "    labels.pop(0)\n",
        "    input = input_ids + answer_ids\n",
        "    input.pop(-1)\n",
        "    return labels, input\n",
        "\n",
        "\n",
        "def pad_collate(batch, padding, length=None):\n",
        "    max_size = max([len(x[\"input_ids\"]) for x in batch]) if not length else length\n",
        "    collated = []\n",
        "\n",
        "    for elem in batch:\n",
        "        copy = {}\n",
        "        copy[\"input_ids\"] = torch.tensor(elem[\"input_ids\"] + [padding] * (max_size - len(elem[\"input_ids\"])))\n",
        "\n",
        "        if elem[\"for_classification\"]:\n",
        "            copy[\"labels\"] = torch.tensor(elem[\"labels\"]).float()\n",
        "        else:\n",
        "            copy[\"labels\"] = torch.tensor(elem[\"labels\"] + [-100] * (max_size - len(elem[\"labels\"])))\n",
        "\n",
        "        if elem[\"segments\"]:\n",
        "            copy[\"segments\"] = torch.tensor(elem[\"segments\"] + [elem[\"segments\"][-1] + 1] * (max_size - len(elem[\"segments\"])))\n",
        "\n",
        "        if elem[\"global_attention_mask\"]:\n",
        "            copy[\"global_attention_mask\"] = torch.tensor(elem[\"global_attention_mask\"] + [False] * (max_size - len(elem[\"global_attention_mask\"])))\n",
        "\n",
        "        copy[\"depth\"] = elem[\"depth\"]\n",
        "        collated.append(copy)\n",
        "\n",
        "    return default_collate(collated)"
      ],
      "metadata": {
        "id": "TEJgo1J7vymh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "from llama_models.llama3.api import Tokenizer\n",
        "\n",
        "adjectives = [\"wrong\", \"glamorous\", \"stormy\", \"weary\", \"witty\", \"tense\", \"pessimistic\", \"frightened\", \"cruel\",\n",
        "              \"helpful\", \"hurt\", \"comfortable\", \"worried\", \"aggressive\", \"blushing\", \"proud\", \"rude\", \"lucky\",\n",
        "              \"fearless\", \"diplomatic\", \"hypocritical\", \"quaint\", \"perfect\", \"jittery\", \"friendly\", \"horrible\",\n",
        "              \"attentive\", \"victorious\", \"naughty\", \"condemned\", \"fancy\", \"zealous\", \"crowded\", \"sincere\", \"busy\",\n",
        "              \"curious\", \"talented\", \"modern\", \"bright\", \"mean\", \"foolish\", \"tender\", \"mysterious\", \"alert\", \"fine\",\n",
        "              \"amused\", \"bad-tempered\", \"lonely\", \"good\", \"muddy\", \"frantic\", \"shy\", \"versatile\", \"loving\", \"elated\",\n",
        "              \"powerful\", \"troubled\", \"easy\", \"innocent\", \"hilarious\", \"strange\", \"disobedient\", \"elegant\", \"joyous\",\n",
        "              \"careless\", \"shiny\", \"adorable\", \"inquisitive\", \"precious\", \"wide-eyed\", \"beautiful\", \"confused\",\n",
        "              \"embarrassed\", \"famous\", \"nervous\", \"plain\", \"distinct\", \"courageous\", \"gleaming\", \"bored\",\n",
        "              \"broad-minded\", \"fragile\", \"outstanding\", \"cooperative\", \"inexpensive\", \"charming\", \"confident\", \"grumpy\",\n",
        "              \"messy\", \"excited\", \"long\", \"talkative\", \"reserved\", \"tame\", \"wandering\", \"different\", \"agreeable\",\n",
        "              \"cute\", \"scared\", \"popular\", \"exuberant\", \"impartial\", \"old-fashioned\", \"clean\", \"helpless\",\n",
        "              \"thoughtless\", \"selfish\", \"serious\", \"homely\", \"worrisome\", \"polite\", \"tired\", \"ugly\", \"light\", \"ugliest\",\n",
        "              \"tidy\", \"sleepy\", \"unpleasant\", \"enchanting\", \"intellectual\", \"combative\", \"rational\", \"uptight\",\n",
        "              \"sensible\", \"supportive\", \"spotless\", \"disgusted\", \"ambitious\", \"pleasant\", \"silly\", \"clumsy\", \"average\",\n",
        "              \"vivacious\", \"gifted\", \"straightforward\", \"smart\", \"impatient\", \"outrageous\", \"calm\", \"gorgeous\", \"frail\",\n",
        "              \"dull\", \"thoughtful\", \"dishonest\", \"difficult\", \"bossy\", \"stubborn\", \"anxious\", \"stupid\", \"attractive\"]\n",
        "rule_block = 200\n",
        "deduction_separator = 201\n",
        "rule_separator = 202\n",
        "fact_block = 203\n",
        "query_block = 204\n",
        "preds_block = 205\n",
        "end_of_turn = 206\n",
        "end_of_text = 207\n",
        "special_tokens = {1: 210, 0: 211}\n",
        "pad = 208\n",
        "\n",
        "\n",
        "def process_atoms(facts):\n",
        "    # return [x for x in facts]\n",
        "    return [adjectives[int(x)] for x in facts]\n",
        "\n",
        "\n",
        "def process_rules(rules):\n",
        "    answer = []\n",
        "    for facts, deduction in rules:\n",
        "        facts = process_atoms(facts)\n",
        "        answer.append(f\"{' and '.join(facts)} is {process_atoms([deduction])[0]}\")\n",
        "    return \". \".join(answer)\n",
        "\n",
        "\n",
        "def llama_tokenize_input(blob, tokenizer: Tokenizer):\n",
        "    from llama_models.llama3.api import ChatFormat\n",
        "\n",
        "    message = f\"\"\"\n",
        "facts: {\", \".join(process_atoms(blob[\"facts\"]))}\n",
        "rules: {process_rules(blob[\"rules\"])}\n",
        "query: {process_atoms(blob[\"query\"][0])[0]}\n",
        "result: \"\"\"\n",
        "    format = ChatFormat(tokenizer)\n",
        "    input_ids = format.encode_content(message).tokens\n",
        "    return input_ids\n",
        "\n",
        "\n",
        "def llama_tokenize_output(blob, tokenizer: Tokenizer, for_classification: bool):\n",
        "    from llama_models.llama3.api import ChatFormat\n",
        "\n",
        "    if for_classification:\n",
        "        return [0, 1] if blob[\"label\"][0] else [1, 0]\n",
        "    else:\n",
        "        answer = \"True\" if blob[\"label\"][0] else \"False\"\n",
        "        format = ChatFormat(tokenizer)\n",
        "        answer_ids = format._encode_content(answer)[0] + [tokenizer.special_tokens[\"<|eom_id|>\"]]\n",
        "        return answer_ids\n",
        "\n",
        "\n",
        "def tokenize_input(blob, global_attention_mask=False):\n",
        "    query = [query_block] + [int(x) for x in blob[\"query\"]]\n",
        "    facts = [fact_block] + [int(x) for i, x in enumerate(blob[\"facts\"])]\n",
        "    preds = [preds_block] + [int(x) for i, x in enumerate(blob[\"preds\"])]\n",
        "\n",
        "    rules = [[int(y) for y in x] + [deduction_separator, int(r)] for i, (x, r) in enumerate(blob[\"rules\"])]\n",
        "    rules = [rule_block] + [x for i, x in enumerate([y for x in rules for y in x])]\n",
        "    input_ids = preds + rules + facts + query + [end_of_turn]\n",
        "\n",
        "    if not global_attention_mask:\n",
        "        return input_ids\n",
        "    else:\n",
        "        return input_ids, [False for _ in input_ids]\n",
        "\n",
        "\n",
        "def tokenize_output(blob, for_classification: bool):\n",
        "    if for_classification:\n",
        "        return [0, 1] if blob[\"label\"][0] else [1, 0]\n",
        "    else:\n",
        "        label = list(map(lambda x: special_tokens[int(x)], blob[\"label\"]))\n",
        "        answer_ids = label + [end_of_text]\n",
        "        return answer_ids\n",
        "\n",
        "\n",
        "segment_by = [deduction_separator, rule_separator, rule_block, fact_block, query_block, end_of_turn, end_of_text, pad]\n",
        "\n",
        "\n",
        "def generate_segments(input_ids):\n",
        "    segments = []\n",
        "    for elem in input_ids:\n",
        "        segments.append(1 if elem in segment_by else 0)\n",
        "    return segments\n",
        "\n",
        "\n",
        "def is_reachable(facts, rules, query):\n",
        "    reachable = set(facts)\n",
        "    changed = True\n",
        "    while changed:\n",
        "        changed = False\n",
        "        for rule in rules:\n",
        "            premises, conclusion = rule\n",
        "            if all(premise in reachable for premise in premises) and conclusion not in reachable:\n",
        "                reachable.add(conclusion)\n",
        "                changed = True\n",
        "    return 1 if query in reachable else 0\n",
        "\n",
        "\n",
        "def process_labels(blob, expand):\n",
        "    blob[\"query\"] = blob[\"preds\"] if expand else [blob[\"query\"]]\n",
        "    blob[\"label\"] = [is_reachable(blob[\"facts\"], blob[\"rules\"], query) for query in blob[\"preds\"]] if expand else [blob[\"label\"]]\n",
        "    return blob\n",
        "\n",
        "\n",
        "def load(file, expand=False, tokenizer=None, for_classification=False):\n",
        "    with open(file, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    processed = [process_labels(x, expand) for x in data]\n",
        "\n",
        "    ds = []\n",
        "    for elem in processed:\n",
        "        global_attention_mask = None\n",
        "        if tokenizer is not None:\n",
        "            inp, out = llama_tokenize_input(elem, tokenizer), llama_tokenize_output(elem, tokenizer, for_classification)\n",
        "        else:\n",
        "            (inp, global_attention_mask), out = tokenize_input(elem, global_attention_mask=True), tokenize_output(elem, for_classification)\n",
        "\n",
        "        # input/output for inference, input_ids/labels for training\n",
        "        if for_classification:\n",
        "            data = {\"input\": inp, \"output\": out, \"input_ids\": inp, \"labels\": out}\n",
        "        else:\n",
        "            labels, input_ids = next_word_prediction_labels(inp, out)\n",
        "            data = {\"input\": inp, \"output\": out, \"input_ids\": input_ids, \"labels\": labels}\n",
        "\n",
        "        if global_attention_mask:\n",
        "            data[\"global_attention_mask\"] = global_attention_mask\n",
        "        else:\n",
        "            data[\"global_attention_mask\"] = None\n",
        "\n",
        "        if not tokenizer:\n",
        "            data[\"segments\"] = generate_segments(data[\"input_ids\"])\n",
        "        else:\n",
        "            data[\"segments\"] = None\n",
        "\n",
        "        data[\"depth\"] = elem[\"depth\"]\n",
        "        data[\"for_classification\"] = for_classification\n",
        "\n",
        "        ds.append(data)\n",
        "    return ds\n",
        "\n",
        "\n",
        "def train_curriculum(ds: list, epoch, select_layer_items=500, non_select_layer_items=50):\n",
        "    samples_by_depth = {i: [] for i in range(7)}\n",
        "    for elem in ds:\n",
        "        samples_by_depth[elem[\"depth\"]].append(elem)\n",
        "    ds.clear()\n",
        "\n",
        "    if epoch >= 7:\n",
        "        dump = []\n",
        "        for i in range(7):\n",
        "            dump.extend(samples_by_depth[i])\n",
        "        random.shuffle(dump)\n",
        "        return dump\n",
        "\n",
        "    train_ds = []\n",
        "    for _ in range(select_layer_items):\n",
        "        train_ds.append(samples_by_depth[epoch].pop(0))\n",
        "    for j in range(epoch):\n",
        "        for _ in range(non_select_layer_items):\n",
        "            train_ds.append(samples_by_depth[j].pop(0))\n",
        "\n",
        "    for i in range(7):\n",
        "        ds.extend(samples_by_depth[i])\n",
        "    return train_ds\n",
        "\n",
        "\n",
        "def select(ds: list, select_items=500):\n",
        "    samples_by_depth = {i: [] for i in range(7)}\n",
        "    for elem in ds:\n",
        "        samples_by_depth[elem[\"depth\"]].append(elem)\n",
        "    ds.clear()\n",
        "\n",
        "    train_ds = []\n",
        "    for key in samples_by_depth.keys():\n",
        "        for _ in range(select_items):\n",
        "            train_ds.append(samples_by_depth[key].pop(0))\n",
        "    for i in range(7):\n",
        "        ds.extend(samples_by_depth[i])\n",
        "    return train_ds\n"
      ],
      "metadata": {
        "id": "s6Trzsqpv-wD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def eval_model(llm_model, inference_ids, vocabulary=special_tokens, answer_position=0):\n",
        "    correct = 0\n",
        "    ood = 0\n",
        "    false_positive = 0\n",
        "    false_negative = 0\n",
        "    positive = 0\n",
        "    negative = 0\n",
        "    correct_by_depth = {}\n",
        "    incorrect_by_depth = {}\n",
        "    for blob in inference_ids:\n",
        "        depth = blob[\"depth\"]\n",
        "        expected = blob[\"output\"][answer_position]\n",
        "        model_input = torch.tensor([blob[\"input\"]]).cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = llm_model(model_input, **blob)\n",
        "            last_token_logits = logits[:, -1, :]\n",
        "            next_token_id = torch.argmax(last_token_logits, dim=-1)\n",
        "\n",
        "        if next_token_id == vocabulary[1]:\n",
        "            positive += 1\n",
        "        if next_token_id == vocabulary[0]:\n",
        "            negative += 1\n",
        "\n",
        "        if next_token_id not in vocabulary.values():\n",
        "            ood += 1\n",
        "        elif expected == next_token_id:\n",
        "            correct += 1\n",
        "            if depth not in correct_by_depth:\n",
        "                correct_by_depth[depth] = 0\n",
        "            correct_by_depth[depth] += 1\n",
        "        else:\n",
        "            if depth not in incorrect_by_depth:\n",
        "                incorrect_by_depth[depth] = 0\n",
        "            incorrect_by_depth[depth] += 1\n",
        "            if next_token_id == vocabulary[1]:\n",
        "                false_positive += 1\n",
        "            else:\n",
        "                false_negative += 1\n",
        "\n",
        "    print(f\"positive predictions: {positive}, negative predictions: {negative}\")\n",
        "    print(f\"correct: {correct}, false positive: {false_positive}, false negative: {false_negative}, ood: {ood}\")\n",
        "    print(f\"correct by depth: {correct_by_depth}\")\n",
        "    print(f\"incorrect by depth: {incorrect_by_depth}\")\n",
        "\n",
        "\n",
        "def visualize_routes(routes):\n",
        "    for route_idx, route in enumerate(routes):\n",
        "        plt.plot(list(range(len(route))), route)\n",
        "\n",
        "    plt.xlabel('Time/Steps')\n",
        "    plt.ylabel('Layer')\n",
        "    plt.title('Layer Visitation over Time')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "dlm0UDxmv2PL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the terms described in the LICENSE file in\n",
        "# top-level folder for each specific model found within the models/ directory at\n",
        "# the top-level of this source tree.\n",
        "\n",
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "# This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n",
        "\n",
        "import math\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, einsum\n",
        "\n",
        "import einx\n",
        "from einops import rearrange, repeat, reduce, pack, unpack\n",
        "\n",
        "\n",
        "class RMSNorm(torch.nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def _norm(self, x):\n",
        "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self._norm(x.float()).type_as(x)\n",
        "        return output * self.weight\n",
        "\n",
        "\n",
        "def apply_scaling(freqs: torch.Tensor):\n",
        "    # RoPE scaling (values obtained from grid search)\n",
        "    scale_factor = 8\n",
        "    low_freq_factor = 1\n",
        "    high_freq_factor = 4\n",
        "    old_context_len = 8192  # original llama3 length\n",
        "\n",
        "    low_freq_wavelen = old_context_len / low_freq_factor\n",
        "    high_freq_wavelen = old_context_len / high_freq_factor\n",
        "    new_freqs = []\n",
        "    for freq in freqs:\n",
        "        wavelen = 2 * math.pi / freq\n",
        "        if wavelen < high_freq_wavelen:\n",
        "            new_freqs.append(freq)\n",
        "        elif wavelen > low_freq_wavelen:\n",
        "            new_freqs.append(freq / scale_factor)\n",
        "        else:\n",
        "            assert low_freq_wavelen != high_freq_wavelen\n",
        "            smooth = (old_context_len / wavelen - low_freq_factor) / (high_freq_factor - low_freq_factor)\n",
        "            new_freqs.append((1 - smooth) * freq / scale_factor + smooth * freq)\n",
        "    return torch.tensor(new_freqs, dtype=freqs.dtype, device=freqs.device)\n",
        "\n",
        "\n",
        "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0, use_scaled: bool = False):\n",
        "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "    t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n",
        "    if use_scaled:\n",
        "        freqs = apply_scaling(freqs)\n",
        "    freqs = torch.outer(t, freqs)\n",
        "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
        "    return freqs_cis\n",
        "\n",
        "\n",
        "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
        "    ndim = x.ndim\n",
        "    assert 0 <= 1 < ndim\n",
        "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
        "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
        "    return freqs_cis.view(*shape)\n",
        "\n",
        "\n",
        "def apply_rotary_emb(xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
        "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
        "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
        "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
        "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
        "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
        "\n",
        "\n",
        "# https://github.com/lucidrains/x-transformers/blob/main/x_transformers/x_transformers.py\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "\n",
        "def default(val, d):\n",
        "    if exists(val):\n",
        "        return val\n",
        "    return d() if callable(d) else d\n",
        "\n",
        "\n",
        "def divisible_by(num, den):\n",
        "    return (num % den) == 0\n",
        "\n",
        "\n",
        "def pad_at_dim(t, pad: tuple[int, int], dim=-1, value=0.):\n",
        "    if pad == (0, 0):\n",
        "        return t\n",
        "\n",
        "    dims_from_right = (- dim - 1) if dim < 0 else (t.ndim - dim - 1)\n",
        "    zeros = ((0, 0) * dims_from_right)\n",
        "    return F.pad(t, (*zeros, *pad), value=value)\n",
        "\n",
        "\n",
        "def l2norm(t, groups=1):\n",
        "    t = rearrange(t, '... (g d) -> ... g d', g=groups)\n",
        "    t = F.normalize(t, p=2, dim=-1)\n",
        "    return rearrange(t, '... g d -> ... (g d)')\n",
        "\n",
        "\n",
        "class always():\n",
        "    def __init__(self, val):\n",
        "        self.val = val\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self.val\n",
        "\n",
        "\n",
        "# positional embeddings\n",
        "\n",
        "class AbsolutePositionalEmbedding(nn.Module):\n",
        "    def __init__(self, dim, max_seq_len, l2norm_embed=False):\n",
        "        super().__init__()\n",
        "        self.scale = dim ** -0.5 if not l2norm_embed else 1.\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.l2norm_embed = l2norm_embed\n",
        "        self.emb = nn.Embedding(max_seq_len, dim)\n",
        "\n",
        "    def forward(self, x, pos=None, seq_start_pos=None):\n",
        "        seq_len, device = x.shape[1], x.device\n",
        "        assert seq_len <= self.max_seq_len, f'you are passing in a sequence length of {seq_len} but your absolute positional embedding has a max sequence length of {self.max_seq_len}'\n",
        "\n",
        "        if not exists(pos):\n",
        "            pos = torch.arange(seq_len, device=device)\n",
        "\n",
        "        if exists(seq_start_pos):\n",
        "            pos = (pos - seq_start_pos[..., None]).clamp(min=0)\n",
        "\n",
        "        pos_emb = self.emb(pos)\n",
        "        pos_emb = pos_emb * self.scale\n",
        "        return l2norm(pos_emb) if self.l2norm_embed else pos_emb\n",
        "\n",
        "\n",
        "class ScaledSinusoidalEmbedding(nn.Module):\n",
        "    def __init__(self, dim, theta=10000):\n",
        "        super().__init__()\n",
        "        assert divisible_by(dim, 2)\n",
        "        self.scale = nn.Parameter(torch.ones(1) * dim ** -0.5)\n",
        "\n",
        "        half_dim = dim // 2\n",
        "        freq_seq = torch.arange(half_dim).float() / half_dim\n",
        "        inv_freq = theta ** -freq_seq\n",
        "        self.register_buffer('inv_freq', inv_freq, persistent=False)\n",
        "\n",
        "    def forward(self, x, pos=None, seq_start_pos=None):\n",
        "        seq_len, device = x.shape[1], x.device\n",
        "\n",
        "        if not exists(pos):\n",
        "            pos = torch.arange(seq_len, device=device)\n",
        "\n",
        "        if exists(seq_start_pos):\n",
        "            pos = pos - seq_start_pos[..., None]\n",
        "\n",
        "        emb = einsum('i, j -> i j', pos, self.inv_freq)\n",
        "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
        "        return emb * self.scale\n",
        "\n",
        "\n",
        "class RelativePositionBias(nn.Module):\n",
        "    def __init__(self, scale, causal=False, num_buckets=32, max_distance=128, heads=8):\n",
        "        super().__init__()\n",
        "        self.scale = scale\n",
        "        self.causal = causal\n",
        "        self.num_buckets = num_buckets\n",
        "        self.max_distance = max_distance\n",
        "        self.relative_attention_bias = nn.Embedding(num_buckets, heads)\n",
        "\n",
        "    @staticmethod\n",
        "    def _relative_position_bucket(relative_position, causal=True, num_buckets=32, max_distance=128):\n",
        "        ret = 0\n",
        "        n = -relative_position\n",
        "        if not causal:\n",
        "            num_buckets //= 2\n",
        "            ret += (n < 0).long() * num_buckets\n",
        "            n = torch.abs(n)\n",
        "        else:\n",
        "            n = torch.max(n, torch.zeros_like(n))\n",
        "\n",
        "        max_exact = num_buckets // 2\n",
        "        is_small = n < max_exact\n",
        "\n",
        "        val_if_large = max_exact + (torch.log(n.float() / max_exact) / math.log(max_distance / max_exact) * (num_buckets - max_exact)).long()\n",
        "        val_if_large = torch.min(val_if_large, torch.full_like(val_if_large, num_buckets - 1))\n",
        "\n",
        "        ret += torch.where(is_small, n, val_if_large)\n",
        "        return ret\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "        return next(self.parameters()).device\n",
        "\n",
        "    def forward(self, i, j):\n",
        "        device = self.device\n",
        "        q_pos = torch.arange(j - i, j, dtype=torch.long, device=device)\n",
        "        k_pos = torch.arange(j, dtype=torch.long, device=device)\n",
        "        rel_pos = einx.subtract('j, i -> i j', k_pos, q_pos)\n",
        "        rp_bucket = self._relative_position_bucket(rel_pos, causal=self.causal, num_buckets=self.num_buckets, max_distance=self.max_distance)\n",
        "        values = self.relative_attention_bias(rp_bucket)\n",
        "        bias = rearrange(values, 'i j h -> h i j')\n",
        "        return bias * self.scale\n",
        "\n",
        "\n",
        "class AlibiPositionalBias(nn.Module):\n",
        "    def __init__(self, heads, total_heads=None, slopes: list[int] | None = None):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.total_heads = default(total_heads, heads)\n",
        "\n",
        "        slopes = torch.Tensor(default(slopes, self._get_slopes(heads)))\n",
        "        slopes = rearrange(slopes, 'h -> h 1 1')\n",
        "\n",
        "        self.register_buffer('slopes', slopes, persistent=False)\n",
        "        self.register_buffer('bias', None, persistent=False)\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "        return next(self.buffers()).device\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_slopes(heads):\n",
        "        def get_slopes_power_of_2(n):\n",
        "            start = (2 ** (-2 ** -(math.log2(n) - 3)))\n",
        "            ratio = start\n",
        "            return [start * ratio ** i for i in range(n)]\n",
        "\n",
        "        if math.log2(heads).is_integer():\n",
        "            return get_slopes_power_of_2(heads)\n",
        "\n",
        "        closest_power_of_2 = 2 ** math.floor(math.log2(heads))\n",
        "        return get_slopes_power_of_2(closest_power_of_2) + get_slopes_power_of_2(2 * closest_power_of_2)[0::2][:heads - closest_power_of_2]\n",
        "\n",
        "    def forward_custom_pos(self, pos_i: torch.Tensor, pos_j: torch.Tensor | None = None):\n",
        "        h, device = self.total_heads, self.device\n",
        "\n",
        "        pos_j = default(pos_j, pos_i)\n",
        "        bias = -einx.subtract('... j, ... i -> ... i j', pos_j, pos_i).abs()\n",
        "\n",
        "        if bias.ndim == 3:\n",
        "            bias = rearrange(bias, 'b i j -> b 1 i j')\n",
        "\n",
        "        bias = bias * self.slopes\n",
        "        num_heads_unalibied = h - bias.shape[-3]\n",
        "        bias = pad_at_dim(bias, (0, num_heads_unalibied), dim=-3)\n",
        "\n",
        "        return bias\n",
        "\n",
        "    def forward(self, i, j):\n",
        "        h, device = self.total_heads, self.device\n",
        "\n",
        "        if exists(self.bias) and self.bias.shape[-1] >= j and self.bias.shape[-2] >= i:\n",
        "            return self.bias[..., -i:, -j:]\n",
        "\n",
        "        seq_arange = torch.arange(j - i, j, device=device)\n",
        "        context_arange = torch.arange(j, device=device)\n",
        "        bias = -einx.subtract('j, i -> 1 i j', context_arange, seq_arange).abs()\n",
        "\n",
        "        bias = bias * self.slopes\n",
        "        num_heads_unalibied = h - bias.shape[-3]\n",
        "        bias = pad_at_dim(bias, (0, num_heads_unalibied), dim=-3)\n",
        "\n",
        "        self.register_buffer('bias', bias, persistent=False)\n",
        "        return self.bias\n",
        "\n",
        "\n",
        "class DynamicPositionBias(nn.Module):\n",
        "    def __init__(self, dim, *, heads, depth, log_distance=False, norm=False):\n",
        "        super().__init__()\n",
        "        assert depth >= 1, 'depth for dynamic position bias MLP must be greater or equal to 1'\n",
        "        self.log_distance = log_distance\n",
        "\n",
        "        self.mlp = nn.ModuleList([])\n",
        "\n",
        "        self.mlp.append(nn.Sequential(nn.Linear(1, dim), nn.LayerNorm(dim) if norm else None, nn.SiLU()))\n",
        "\n",
        "        for _ in range(depth - 1):\n",
        "            self.mlp.append(nn.Sequential(nn.Linear(dim, dim), nn.LayerNorm(dim) if norm else None, nn.SiLU()))\n",
        "\n",
        "        self.mlp.append(nn.Linear(dim, heads))\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "        return next(self.parameters()).device\n",
        "\n",
        "    def forward(self, i, j):\n",
        "        assert i == j\n",
        "        n, device = j, self.device\n",
        "\n",
        "        # get the (n x n) matrix of distances\n",
        "        seq_arange = torch.arange(n, device=device)\n",
        "        context_arange = torch.arange(n, device=device)\n",
        "        indices = einx.subtract('i, j -> i j', seq_arange, context_arange)\n",
        "        indices += (n - 1)\n",
        "\n",
        "        # input to continuous positions MLP\n",
        "        pos = torch.arange(-n + 1, n, device=device).bfloat16()\n",
        "        pos = rearrange(pos, '... -> ... 1')\n",
        "\n",
        "        if self.log_distance:\n",
        "            pos = torch.sign(pos) * torch.log(pos.abs() + 1)  # log of distance is sign(rel_pos) * log(abs(rel_pos) + 1)\n",
        "\n",
        "        for layer in self.mlp:\n",
        "            pos = layer(pos)\n",
        "\n",
        "        # get position biases\n",
        "        bias = pos[indices]\n",
        "        bias = rearrange(bias, 'i j h -> h i j')\n",
        "        return bias\n",
        "\n",
        "\n",
        "# designed for causal\n",
        "class CoPE(nn.Module):\n",
        "    \"\"\"\n",
        "    Appendix B of https://arxiv.org/abs/2405.18719\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            dim,\n",
        "            heads,\n",
        "            max_pos,\n",
        "            soft_onehot=False,\n",
        "            talking_heads=False,\n",
        "            soft_onehot_temp=5e-2\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.max_pos = max_pos\n",
        "        self.pos_emb = nn.Parameter(torch.zeros(max_pos, dim))\n",
        "\n",
        "        self.talking_heads = nn.Conv2d(heads, heads, 1, bias=False) if talking_heads else None\n",
        "        self.soft_onehot = soft_onehot\n",
        "        self.soft_onehot_temp = soft_onehot_temp\n",
        "\n",
        "        if not soft_onehot:\n",
        "            return\n",
        "\n",
        "        self.register_buffer('positions', torch.arange(max_pos))\n",
        "\n",
        "    def forward(self, query, attn_logits):\n",
        "\n",
        "        if exists(self.talking_heads):\n",
        "            i, j = attn_logits.shape[-2:]\n",
        "            causal_mask = attn_logits.new_ones(i, j).triu_(j - i + 1).bool()\n",
        "\n",
        "            attn_logits = self.talking_heads(attn_logits)\n",
        "\n",
        "            attn_logits = attn_logits.masked_fill(causal_mask, -torch.finfo(attn_logits.dtype).max)\n",
        "\n",
        "        # compute positions\n",
        "\n",
        "        gates = attn_logits.sigmoid()\n",
        "\n",
        "        pos = gates.flip(-1).cumsum(dim=-1).flip(-1)\n",
        "        pos = pos.clamp(max=self.max_pos - 1)\n",
        "\n",
        "        logits_int = einsum('b h n d, p d -> b h n p', query, self.pos_emb)\n",
        "\n",
        "        if self.soft_onehot:\n",
        "            diff_pos = einx.subtract('i, j -> i j', pos, self.positions).abs()\n",
        "            soft_onehot_pos = F.softmax(-diff_pos / self.soft_onehot_temp, dim=-1)\n",
        "            cope_pos_emb = einsum('b h i j p, b h i p -> b h i j', soft_onehot_pos, logits_int)\n",
        "        else:\n",
        "            # interpolate from integer positions\n",
        "            pos_ceil = pos.ceil().long()\n",
        "            pos_floor = pos.floor().long()\n",
        "            logits_ceil = logits_int.gather(-1, pos_ceil)\n",
        "            logits_floor = logits_int.gather(-1, pos_floor)\n",
        "\n",
        "            w = pos - pos_floor\n",
        "            cope_pos_emb = logits_ceil * w + logits_floor * (1 - w)\n",
        "\n",
        "        return cope_pos_emb\n",
        "\n",
        "\n",
        "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
        "    \"\"\"torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\"\n",
        "    bs, slen, n_kv_heads, head_dim = x.shape\n",
        "    if n_rep == 1:\n",
        "        return x\n",
        "    return (\n",
        "        x[:, :, :, None, :]\n",
        "        .expand(bs, slen, n_kv_heads, n_rep, head_dim)\n",
        "        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n",
        "    )\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
        "        self.n_local_heads = args.n_heads\n",
        "        self.n_local_kv_heads = self.n_kv_heads\n",
        "        self.alibi_heads = self.n_local_kv_heads\n",
        "        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n",
        "        self.head_dim = args.dim // args.n_heads\n",
        "        self.dim = args.dim\n",
        "\n",
        "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
        "        self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
        "        self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
        "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
        "\n",
        "        assert self.alibi_heads <= self.n_local_heads, 'number of ALiBi heads must be less than the total number of heads'\n",
        "        # self.rel_pos = AlibiPositionalBias(heads=self.alibi_heads, total_heads=self.n_local_heads)\n",
        "        # self.rel_pos = DynamicPositionBias(dim=self.dim // 4, heads=self.n_local_heads, log_distance=False, depth=2, norm=True)\n",
        "        # self.rel_pos = RelativePositionBias(scale=self.head_dim ** 0.5, causal=False, heads=self.n_local_heads, num_buckets=32, max_distance=128)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
        "        bsz, seqlen, _ = x.shape\n",
        "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
        "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
        "        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
        "        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
        "\n",
        "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n",
        "\n",
        "        # repeat k/v heads if n_kv_heads < n_heads (GQA)\n",
        "        xk = repeat_kv(xk, self.n_rep)  # (bs, cache_len + seqlen, n_local_heads, head_dim)\n",
        "        xv = repeat_kv(xv, self.n_rep)  # (bs, cache_len + seqlen, n_local_heads, head_dim)\n",
        "        # make heads be a batch dim\n",
        "        xq, xk, xv = (x.transpose(1, 2) for x in (xq, xk, xv))\n",
        "\n",
        "        scores = torch.matmul(xq, xk.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
        "        # scores = scores + self.rel_pos(xq.shape[-2], xk.shape[-2]).to(scores)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores + mask.unsqueeze(1)  # (bs, n_local_heads, seqlen, cache_len + seqlen)\n",
        "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
        "        output = torch.matmul(scores, xv)  # (bs, n_local_heads, seqlen, head_dim)\n",
        "\n",
        "        # concatenate all the heads\n",
        "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
        "        # output projection\n",
        "        proj = self.wo(output)\n",
        "        return proj\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim: int, hidden_dim: int, multiple_of: int, ffn_dim_multiplier: Optional[float]):\n",
        "        super().__init__()\n",
        "        hidden_dim = int(2 * hidden_dim / 3)\n",
        "        # custom dim factor multiplier\n",
        "        if ffn_dim_multiplier is not None:\n",
        "            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n",
        "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
        "\n",
        "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
        "        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, layer_id: int, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.n_heads = args.n_heads\n",
        "        self.dim = args.dim\n",
        "        self.head_dim = args.dim // args.n_heads\n",
        "        self.attention = Attention(args)\n",
        "        self.feed_forward = FeedForward(\n",
        "            dim=args.dim,\n",
        "            hidden_dim=4 * args.dim,\n",
        "            multiple_of=args.multiple_of,\n",
        "            ffn_dim_multiplier=args.ffn_dim_multiplier,\n",
        "        )\n",
        "        # self.gate = nn.Sequential(\n",
        "        #     nn.Linear(args.dim, args.dim, bias=False),\n",
        "        #     nn.SiLU(),\n",
        "        #     nn.Linear(args.dim, 1, bias=False),\n",
        "        #     nn.Sigmoid()\n",
        "        # )\n",
        "        self.layer_id = layer_id\n",
        "        self.attention_norm = RMSNorm(args.dim, eps=1e-05)\n",
        "        self.ffn_norm = RMSNorm(args.dim, eps=1e-05)\n",
        "        self.gate_norm = RMSNorm(args.dim, eps=1e-05)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
        "        attention_out = self.attention(self.attention_norm(x), freqs_cis, mask=mask)\n",
        "        # gate_out = self.gate(self.gate_norm(attention_out))\n",
        "        h = x + attention_out # * gate_out\n",
        "        out = h + self.feed_forward(self.ffn_norm(h))\n",
        "        return out#, gate_out.squeeze(-1)\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, params: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.params = params\n",
        "        self.vocab_size = params.vocab_size\n",
        "        self.n_layers = params.n_layers\n",
        "\n",
        "        self.tok_embeddings = nn.Embedding(params.vocab_size, params.dim)\n",
        "        self.meta_tokens = params.meta_embeddings\n",
        "        if self.meta_tokens:\n",
        "            self.meta_token_embeddings = torch.nn.Parameter(torch.randn(params.meta_embeddings, params.dim))\n",
        "\n",
        "        nn.init.normal_(self.tok_embeddings.weight, mean=0.0, std=1.0 / params.dim ** 0.5)\n",
        "        # self.positional_embeddings = ScaledSinusoidalEmbedding(params.dim)\n",
        "        # self.positional_embeddings = AbsolutePositionalEmbedding(params.dim, params.max_seq_len, l2norm_embed=False)\n",
        "        self.positional_embeddings = always(0)\n",
        "\n",
        "        self.layers = torch.nn.ModuleList()\n",
        "        for layer_id in range(params.n_layers):\n",
        "            self.layers.append(TransformerBlock(layer_id, params))\n",
        "\n",
        "        self.norm = RMSNorm(params.dim, eps=1e-05)\n",
        "        self.output = nn.Linear(params.dim, params.output_size if params.output_size else params.vocab_size, bias=False)\n",
        "\n",
        "        self.freqs_cis = precompute_freqs_cis(params.dim // params.n_heads, params.max_seq_len * 2, 500000.0, True)\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def forward(self, tokens: torch.Tensor, labels=None, depths=None, global_attention_mask: Optional[torch.Tensor] = None):\n",
        "        return self.hierarchical_forward(tokens, initial_labels=labels, depths=depths, global_attention_mask=global_attention_mask, is_train=False)\n",
        "\n",
        "    def forward_train(self, tokens: torch.Tensor, labels: torch.Tensor, depths, global_attention_mask: Optional[torch.Tensor] = None):\n",
        "        return self.hierarchical_forward(tokens, initial_labels=labels, depths=depths, global_attention_mask=global_attention_mask, is_train=True)\n",
        "\n",
        "    def hierarchical_forward(self, tokens: torch.Tensor, initial_labels: torch.Tensor = None, depths: torch.Tensor = None, global_attention_mask: Optional[torch.Tensor] = None, is_train: bool = None):\n",
        "        _bsz, seqlen = tokens.shape\n",
        "        seqlen += self.meta_tokens\n",
        "\n",
        "        ignore_labels = torch.full((_bsz, self.meta_tokens), -100, dtype=torch.int, device=initial_labels.device)\n",
        "        labels = torch.cat([ignore_labels, initial_labels], dim=1)\n",
        "\n",
        "        e = self.tok_embeddings(tokens)\n",
        "        h = torch.cat([self.meta_token_embeddings.unsqueeze(0).expand(_bsz, -1, -1), e], dim=1) if self.meta_tokens else e\n",
        "        h = h + self.positional_embeddings(h)\n",
        "\n",
        "        self.freqs_cis = self.freqs_cis.clone().to(h.device)\n",
        "        freqs_cis = self.freqs_cis[:seqlen]\n",
        "\n",
        "        mask = torch.full((seqlen, seqlen), float(\"-inf\"), device=tokens.device)\n",
        "        mask = torch.triu(mask, diagonal=1)\n",
        "        mask = mask.type_as(h)\n",
        "\n",
        "        # mask, w = torch.ones(seqlen, seqlen, device=tokens.device), 31 # needs to be odd\n",
        "        # mask = torch.triu(mask, diagonal=-w // 2) - torch.triu(mask, diagonal=w // 2 + 1)\n",
        "        # mask = torch.tril(mask)\n",
        "        # mask[mask == 0] = float(\"-inf\")\n",
        "\n",
        "        if global_attention_mask is not None:\n",
        "            meta_mask = torch.ones(_bsz, self.meta_tokens, dtype=torch.bool, device=global_attention_mask.device)\n",
        "            mask = merge_masks(mask, torch.cat([meta_mask, global_attention_mask], dim=1), labels)\n",
        "        else:\n",
        "            mask = mask.unsqueeze(0).repeat(_bsz, 1, 1)\n",
        "\n",
        "        current_layer_indexes = torch.zeros(_bsz, device=tokens.device).type(dtype=torch.long)\n",
        "        invocations = torch.zeros(_bsz, device=tokens.device).type(dtype=torch.long)\n",
        "        history = []\n",
        "        all_next_layer_probs = [[] for _ in range(tokens.shape[0])]\n",
        "\n",
        "        while True:\n",
        "            history.append(current_layer_indexes.clone().detach())\n",
        "            active_sequences_mask = (current_layer_indexes != self.params.n_layers)\n",
        "\n",
        "            if not active_sequences_mask.any():\n",
        "                break\n",
        "\n",
        "            unique_layer_indices = torch.unique(current_layer_indexes[active_sequences_mask])\n",
        "\n",
        "            for layer_index in unique_layer_indices:\n",
        "                i = layer_index.item()\n",
        "                this_batch = (current_layer_indexes == i) & active_sequences_mask\n",
        "                batch_h = h[this_batch]\n",
        "                batch_mask = mask[this_batch]\n",
        "                layer = self.layers[i]\n",
        "                # h[this_batch], next_layer_probs = layer(batch_h, freqs_cis, mask=batch_mask)\n",
        "                h[this_batch] = layer(batch_h, freqs_cis, mask=batch_mask)\n",
        "\n",
        "                # last_layer_prob = 0.8\n",
        "                # step = 0.2\n",
        "                # layer_threshold = last_layer_prob - (self.params.n_layers * step) + step * (i + 1)\n",
        "                layer_threshold = 0.7\n",
        "                training_stability = -0.05 * invocations\n",
        "                layer_threshold = training_stability + layer_threshold\n",
        "\n",
        "                # if labels is None or not is_train:\n",
        "                #     layer_probs = next_layer_probs[:, -1]\n",
        "                #     increment_decrement = torch.where(layer_probs > layer_threshold[this_batch], 1, -1)\n",
        "                # else:\n",
        "                #     layer_probs = torch.where(labels[this_batch] != -100, next_layer_probs, 2)\n",
        "                #     increment_decrement = torch.all(layer_probs > layer_threshold[this_batch].unsqueeze(1), dim=1).long() * 2 - 1\n",
        "                #\n",
        "                # for i, val in enumerate(this_batch):\n",
        "                #     if val:\n",
        "                #         probs = layer_probs[0].unsqueeze(0)\n",
        "                #         all_next_layer_probs[i].append(probs[probs <= 1.1])\n",
        "                #         layer_probs = layer_probs[1:]\n",
        "\n",
        "                if depths is not None:\n",
        "                    pos = invocations[this_batch]\n",
        "                    depth = depths[this_batch]\n",
        "                    increment_decrement = torch.tensor([is_forward_at_position_for_depth(p.item(), d.item()) for p, d in zip(pos, depth)], device=tokens.device)\n",
        "\n",
        "                current_layer_indexes[this_batch] += increment_decrement\n",
        "                current_layer_indexes = current_layer_indexes.clamp(min=0)\n",
        "\n",
        "                invocations[this_batch] += 1\n",
        "\n",
        "        h = self.norm(h)\n",
        "        output = self.output(h).float()\n",
        "        return output, labels, all_next_layer_probs, history\n",
        "\n",
        "\n",
        "def merge_masks(local_mask: torch.Tensor, global_attention_mask: torch.Tensor, labels: torch.Tensor):\n",
        "    batch_size, seq_len = global_attention_mask.size()\n",
        "    expanded_local_mask = local_mask.unsqueeze(0).expand(batch_size, -1, -1)\n",
        "\n",
        "    global_mask = global_attention_mask[:, :, None] + global_attention_mask[:, None, :]\n",
        "\n",
        "    first_label_indices = (labels != -100).long().argmax(dim=1)\n",
        "    pad_mask = torch.arange(seq_len, device=labels.device)[None, :] > first_label_indices[:, None]\n",
        "\n",
        "    global_mask[pad_mask.unsqueeze(1).expand(-1, seq_len, -1)] = 0\n",
        "\n",
        "    combined_mask = expanded_local_mask.clone()\n",
        "    combined_mask[global_mask > 0.1] = 1\n",
        "    return combined_mask\n",
        "\n",
        "\n",
        "def is_forward_at_position_for_depth(pos, depth):\n",
        "    seq = [1, 1, 1] #+ [-1, -1, 1, 1] * max(0, depth - 1) + [1]\n",
        "    return seq[min(pos, len(seq) - 1)]\n"
      ],
      "metadata": {
        "id": "DMfV73GPv4zs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from dataclasses import dataclass\n",
        "from dataclasses_json import dataclass_json\n",
        "from typing import Optional, Callable\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def create_model(root_path, args_class, model_class, state_dict_location=None):\n",
        "    conf_path = root_path + \"/params.json\"\n",
        "    with open(conf_path, 'r') as file:\n",
        "        conf = args_class.from_json(file.read())\n",
        "        print(conf)\n",
        "\n",
        "    with torch.device('cpu'):\n",
        "        llm_model = model_class(params=conf)\n",
        "        if state_dict_location is not None:\n",
        "            state_dict = torch.load(state_dict_location, map_location=torch.device(\"cpu\"), weights_only=True)\n",
        "            res = llm_model.load_state_dict(state_dict, strict=False, assign=True)\n",
        "            print(res)\n",
        "\n",
        "    return llm_model\n",
        "\n",
        "\n",
        "@dataclass_json\n",
        "@dataclass\n",
        "class TrainConf:\n",
        "    epochs: int = 1\n",
        "    accumulation_batches: int = 1\n",
        "    optimizer: torch.optim.Optimizer = None\n",
        "    scheduler: torch.optim.lr_scheduler.LRScheduler = None\n",
        "    loss_fn: Callable[..., torch.Tensor] = None\n",
        "    ds_loader: Callable[[list, int, int], DataLoader] = None\n",
        "    eval_model: Optional[Callable[[nn.Module, int], None]] = None\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        for k, v in kwargs.items():\n",
        "            if hasattr(self, k):\n",
        "                setattr(self, k, v)\n",
        "\n",
        "\n",
        "def get_total_grad_norm(llm_model):\n",
        "    total_norm = 0\n",
        "    for name, p in llm_model.named_parameters():\n",
        "        if p.grad is None:\n",
        "            continue\n",
        "        param_norm = p.grad.data.norm(2)\n",
        "        total_norm += param_norm.item() ** 2\n",
        "    total_norm = total_norm ** 0.5\n",
        "    return total_norm\n",
        "\n",
        "\n",
        "def call_model(llm, batch, loss_fn):\n",
        "    input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
        "    labels = batch[\"labels\"].to(\"cuda\")\n",
        "\n",
        "    llm_output = llm.forward_train(input_ids)\n",
        "    return loss_fn(labels, llm_output)\n",
        "\n",
        "\n",
        "def train(conf: TrainConf, llm_model: nn.Module, train_ds: list, validation_ds: list, model_call=call_model):\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "    training_loss_curves = []\n",
        "\n",
        "    train_length = len(train_ds)\n",
        "    validation_length = len(validation_ds)\n",
        "\n",
        "    for epoch in range(conf.epochs):\n",
        "        data_loader = conf.ds_loader(train_ds, train_length, epoch)\n",
        "        validation_loader = conf.ds_loader(validation_ds, validation_length, epoch)\n",
        "        print(f\"\\nEPOCH {epoch}: len(train)={len(data_loader)}, len(validation)={len(validation_loader)}\")\n",
        "\n",
        "        llm_model.train()\n",
        "        # conf.optimizer.train()\n",
        "        training_loss_curve = []\n",
        "        gradient_norm = []\n",
        "\n",
        "        for i, batch in enumerate(data_loader):\n",
        "            loss = model_call(llm_model, batch, conf.loss_fn)\n",
        "            loss.backward()\n",
        "\n",
        "            total_norm = get_total_grad_norm(llm_model)\n",
        "            gradient_norm.append(round(total_norm ** 0.5, 4))\n",
        "\n",
        "            if (i + 1) % conf.accumulation_batches == 0:\n",
        "                conf.optimizer.step()\n",
        "                conf.optimizer.zero_grad()\n",
        "            training_loss_curve.append(round(loss.item() / len(batch), 4))\n",
        "\n",
        "        avg_tloss = round(sum(training_loss_curve) / len(training_loss_curve), 4)\n",
        "        training_loss_curves.append(training_loss_curve)\n",
        "\n",
        "        running_vloss = 0.0\n",
        "        llm_model.eval()\n",
        "        # conf.optimizer.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, batch in enumerate(validation_loader):\n",
        "                loss = model_call(llm_model, batch, conf.loss_fn)\n",
        "                running_vloss += loss.item()\n",
        "        avg_vloss = round(running_vloss / len(validation_loader), 4)\n",
        "        print(f\"LOSS train {avg_tloss} valid {avg_vloss} lr {conf.scheduler.get_last_lr()} \"\n",
        "              f\"min gradient norm: {min(gradient_norm)} max gradient norm: {max(gradient_norm)}\")\n",
        "\n",
        "        if conf.eval_model is not None:\n",
        "            conf.eval_model(llm_model, epoch)\n",
        "\n",
        "        conf.scheduler.step()\n",
        "\n",
        "    return training_loss_curves\n",
        "\n",
        "\n",
        "def visualize(nested_losses):\n",
        "    flattened_list = []\n",
        "    indices = []\n",
        "    current_index = 1\n",
        "    for sublist in nested_losses:\n",
        "        flattened_list.extend(sublist)\n",
        "        indices.extend([current_index] * len(sublist))\n",
        "        current_index += 1\n",
        "\n",
        "    total_items = len(flattened_list)\n",
        "    step = max(total_items // 1000, 1)\n",
        "\n",
        "    selected_items = flattened_list[step - 1::step]\n",
        "    selected_indices = indices[step - 1::step]\n",
        "\n",
        "    x_values = range(1, len(selected_items) + 1)\n",
        "\n",
        "    plt.scatter(x_values, selected_items, c=selected_indices, cmap='viridis')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.yscale(\"log\")\n",
        "    plt.title('Loss curve')\n",
        "    plt.colorbar(label='Epoch')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "GxBYCzUbxiru"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for_classification = False\n",
        "tokenizer_path = \"/content/drive/MyDrive/ML/Llama3.2-3B-Instruct/tokenizer.model\"\n",
        "tokenizer = Tokenizer(tokenizer_path)\n",
        "\n",
        "train_ds = load(\"/content/drive/MyDrive/ML/predicate_logic/train/7k/prop_examples_lp.txt\", for_classification=for_classification, tokenizer=tokenizer)\n",
        "validation_ds = load(\"/content/drive/MyDrive/ML/predicate_logic/validation/prop_examples_lp.txt\", for_classification=for_classification, tokenizer=tokenizer)\n",
        "inference_ids_lp = load(\"/content/drive/MyDrive/ML/predicate_logic/validation/prop_examples_lp.txt\", for_classification=for_classification, tokenizer=tokenizer)\n",
        "inference_ids_lp_star = load(\"/content/drive/MyDrive/ML/predicate_logic/validation/prop_examples_lp_star.txt\", for_classification=for_classification, tokenizer=tokenizer)\n",
        "inference_ids_rp = load(\"/content/drive/MyDrive/ML/predicate_logic/validation/prop_examples_rp.txt\", for_classification=for_classification, tokenizer=tokenizer)\n",
        "\n",
        "def ds_loader(ds, ds_length, epoch):\n",
        "    # curriculum = processor.train_curriculum(ds, epoch, select_layer_items=ds_length // 7, non_select_layer_items=ds_length // 140)\n",
        "    return DataLoader(ds, shuffle=True, batch_size=4, collate_fn=lambda x: pad_collate(x, padding=pad))\n",
        "\n",
        "def loss_fn(llm_output, depth):\n",
        "    logits, labels, gating, history = llm_output\n",
        "\n",
        "    if for_classification:\n",
        "        logit = logits[:, -1]\n",
        "        total_loss = F.binary_cross_entropy_with_logits(logit, labels)\n",
        "    else:\n",
        "        total_loss = F.cross_entropy(logits.transpose(1, 2), labels, label_smoothing=0.)\n",
        "\n",
        "    # layers_traversed_loss = 0.0\n",
        "    # for next_layer_prob in gating:\n",
        "    #     layers_traversed_loss += F.binary_cross_entropy(next_layer_prob, torch.ones_like(next_layer_prob))\n",
        "\n",
        "    # # whatever you think, be confident in it\n",
        "    # confidence_loss = 0.0\n",
        "    # for next_layer_prob in gating:\n",
        "    #     confidence = 0.5 - torch.abs(next_layer_prob - 0.5)\n",
        "    #     confidence_loss += confidence.mean()\n",
        "    # confidence_loss *= 0.01\n",
        "    # confidence_loss /= len(gating)\n",
        "    # total_loss += confidence_loss\n",
        "\n",
        "    # fixed routing\n",
        "    # total_routing_loss = 0\n",
        "    # for pos, gates in enumerate(gating):\n",
        "    #     routing_loss = 0\n",
        "    #     for i in range(len(gates)):\n",
        "    #         next_layer_prob = gates[i]\n",
        "    #         is_forward = is_forward_at_position_for_depth(i, depth[pos])\n",
        "    #         target_prob = torch.ones_like(next_layer_prob) if is_forward > 0 else torch.zeros_like(next_layer_prob)\n",
        "    #         routing_loss += F.binary_cross_entropy(next_layer_prob, target_prob)\n",
        "    #     total_routing_loss += routing_loss / len(gates)\n",
        "    # total_loss += total_routing_loss / len(gating)\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "def inference_model(llm_model, epoch=0):\n",
        "    if for_classification:\n",
        "        vocabulary, answer_position = {1: 1, 0: 0}, 1\n",
        "    else:\n",
        "        vocabulary, answer_position = special_tokens, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        routes = []\n",
        "        def model_invocation(xs, labels, depth, **kwargs):\n",
        "            labels = torch.tensor([labels]).to(\"cuda\")\n",
        "            depths = torch.tensor([depth]).to(\"cuda\")\n",
        "            logits, labels, gates, history = llm_model(xs, labels=labels, depths=depths)\n",
        "            routes.append([x.item() for x in history])\n",
        "            return logits\n",
        "        # eval_model(model_invocation, [x for x in inference_ids if x[\"depth\"] <= epoch], vocabulary=vocabulary, answer_position=answer_position)\n",
        "        visualize_routes(routes)\n",
        "        eval_model(model_invocation, inference_ids_lp, vocabulary=vocabulary, answer_position=answer_position)\n",
        "        eval_model(model_invocation, inference_ids_lp_star, vocabulary=vocabulary, answer_position=answer_position)\n",
        "        eval_model(model_invocation, inference_ids_rp, vocabulary=vocabulary, answer_position=answer_position)\n",
        "\n",
        "def call_model(llm, batch, loss_fn):\n",
        "    input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
        "    labels = batch[\"labels\"].to(\"cuda\")\n",
        "    depths = batch[\"depth\"].to(\"cuda\")\n",
        "\n",
        "    llm_output = llm.forward_train(input_ids, None if for_classification else labels, depths=depths)\n",
        "    return loss_fn(llm_output, depths)\n",
        "\n",
        "root_path = \"/content/drive/MyDrive/ML/predicate_logic/llama_predicate_logic_hierarchical_routing\"\n",
        "state_dict_location = \"/content/drive/MyDrive/ML/Llama3.2-3B-Instruct/consolidated.00.pth\"\n",
        "llm_model = create_model(root_path, ModelArgs, Transformer, state_dict_location=state_dict_location)\n",
        "for name, parameter in llm_model.named_parameters():\n",
        "    parameter.requires_grad = True\n",
        "    parameter.data = parameter.data.bfloat16().to(\"cuda\")\n",
        "    print(f\"{name} (device={parameter.device}, requires_grad={parameter.requires_grad}, dtype={parameter.dtype})\")\n",
        "\n",
        "# optimizer = AdamWScheduleFree(llm_model.parameters(), weight_decay=0.05, betas=(0.9, 0.98), lr=1e-4, warmup_steps=2000)\n",
        "optimizer = torch.optim.AdamW(llm_model.parameters(), weight_decay=0.1, betas=(0.9, 0.999), lr=1e-5)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
        "train_conf = TrainConf(epochs=5, optimizer=optimizer, scheduler=scheduler, loss_fn=loss_fn, ds_loader=ds_loader, eval_model=inference_model)\n",
        "\n",
        "visualize(train(train_conf, llm_model, train_ds, validation_ds, model_call=call_model))"
      ],
      "metadata": {
        "id": "AS1WZ0xUsJgp",
        "outputId": "288ca7b5-b6f9-44b6-b99b-569b21a5ee1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ModelArgs(dim=3072, n_layers=28, n_heads=24, n_kv_heads=8, vocab_size=128256, output_size=128256, multiple_of=256, ffn_dim_multiplier=1.0, meta_embeddings=0, max_batch_size=32, max_seq_len=1024)\n"
          ]
        }
      ]
    }
  ]
}